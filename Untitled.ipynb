{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f12d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk \n",
    "import re\n",
    "df = pd.read_excel('Input.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8db6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative words\n",
    "with open(\"MasterDictionary/positive-words.txt\", \"r\") as file:\n",
    "    positive = file.read().replace('\\n', ' ')\n",
    "#positive words\n",
    "with open(\"MasterDictionary/negative-words.txt\", \"r\", encoding='ISO-8859-1') as file:\n",
    "    negative = file.read().replace('\\n', ' ')\n",
    "\n",
    "#stopwords\n",
    "with open(\"StopWords/StopWords_Auditor.txt\", \"r\",encoding='ISO-8859-1') as file:\n",
    "    StopWords_Auditor = file.read().split('\\n')\n",
    "with open(\"StopWords/StopWords_Currencies.txt\", \"r\" ,encoding='ISO-8859-1') as file:\n",
    "    StopWords_Currencies = file.read().split('\\n')\n",
    "\n",
    "with open(\"StopWords/StopWords_DatesandNumbers.txt\", \"r\" ,encoding='ISO-8859-1') as file:\n",
    "    StopWords_DatesandNumbers = file.read().split('\\n')\n",
    "    \n",
    "    \n",
    "with open(\"StopWords/StopWords_Generic.txt\", \"r\" ,encoding='ISO-8859-1') as file:\n",
    "    StopWords_Generic = file.read().split('\\n')\n",
    "    \n",
    "    \n",
    "with open(\"StopWords/StopWords_GenericLong.txt\", \"r\" ,encoding='ISO-8859-1') as file:\n",
    "    StopWords_GenericLong = file.read().split('\\n')\n",
    "    \n",
    "    \n",
    "with open(\"StopWords/StopWords_Geographic.txt\", \"r\" ,encoding='ISO-8859-1') as file:\n",
    "    StopWords_Geographic = file.read().split('\\n')\n",
    "    \n",
    "with open(\"StopWords/StopWords_Names.txt\", \"r\" ,encoding='ISO-8859-1') as file:\n",
    "    StopWords_Names = file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e84bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_DatesandNumbers = [term.split('|')[0].strip() for term in StopWords_DatesandNumbers]\n",
    "StopWords_Currencies = [term.split('|')[0].strip() for term in StopWords_Currencies]\n",
    "StopWords_Geographic = [term.split('|')[0].strip() for term in StopWords_Geographic]\n",
    "StopWords_Names = [term.split('|')[0].strip() for term in StopWords_Names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca00e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = StopWords_Auditor +StopWords_Currencies+StopWords_DatesandNumbers+StopWords_Generic+StopWords_GenericLong+StopWords_Geographic+StopWords_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f07afeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47a5f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e9d3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos_neg(cleaned_text,pos_neg):\n",
    "    pos_neg_count = 0\n",
    "    for i in cleaned_text.split():\n",
    "        if i in pos_neg.split():\n",
    "            pos_neg_count +=1\n",
    "    return pos_neg_count\n",
    "\n",
    "def get_text(url , stopwords):\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers).content, \"html.parser\")\n",
    "    text = soup.find(\"div\", {\"class\": \"td-post-content\"})\n",
    "    if text is not None:\n",
    "        text = str(text.text)\n",
    "        cleaned_text = re.sub(r'[\\n\\xa0]', '', text)\n",
    "        pattern = r'\\b(?:{})\\b'.format('|'.join(map(re.escape, stopwords)))\n",
    "        cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "        text = re.sub(r'[?!,.]', '', cleaned_text)\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        return cleaned_text , text\n",
    "    else:\n",
    "        \n",
    "        return \"page not found.\",\"0\"\n",
    "\n",
    "def save(cleaned_text,text ,count):\n",
    "    file_name=df['URL_ID'][count]\n",
    "    Positive_Score = count_pos_neg(cleaned_text , positive)\n",
    "    Negative_Score = count_pos_neg(cleaned_text , negative)\n",
    "    POLARITY_SCORE=(Positive_Score - Negative_Score)/ ((Positive_Score + Negative_Score) + 0.000001)\n",
    "    Subjectivity_Score = (Positive_Score + Negative_Score)/ ((len(cleaned_text.split())) + 0.000001)\n",
    "    Average_Sentence_Length = len(cleaned_text.split('.')) / len(cleaned_text.split())\n",
    "    Percentage_of_Complex_words = len(text.split()) / len(cleaned_text.split()) \n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "    \n",
    "\n",
    "    if 'URL_ID' not in text_analysis:\n",
    "        text_analysis['URL_ID'] = []\n",
    "    text_analysis['URL_ID'].append(df['URL_ID'][count])\n",
    "    if 'URL' not in text_analysis:\n",
    "        text_analysis['URL_ID'] = []\n",
    "    text_analysis['URL_ID'].append(df['URL_ID'][count])\n",
    "    if 'Positive_Score' not in text_analysis:\n",
    "        text_analysis['Positive_Score'] = []\n",
    "    text_analysis['Positive_Score'].append(Positive_Score)\n",
    "    if 'Negative_Score' not in text_analysis:\n",
    "        text_analysis['Negative_Score'] = []\n",
    "    text_analysis['Negative_Score'].append(Negative_Score)\n",
    "    if 'POLARITY_SCORE' not in text_analysis:\n",
    "        text_analysis['POLARITY_SCORE'] = []\n",
    "    text_analysis['POLARITY_SCORE'].append(POLARITY_SCORE)\n",
    "    if 'Subjectivity_Score' not in text_analysis:\n",
    "        text_analysis['Subjectivity_Score'] = []\n",
    "    text_analysis['Subjectivity_Score'].append(Subjectivity_Score)\n",
    "    if 'Average_Sentence_Length' not in text_analysis:\n",
    "        text_analysis['Average_Sentence_Length'] = []\n",
    "    text_analysis['Average_Sentence_Length'].append(Average_Sentence_Length)\n",
    "    if 'Percentage_of_Complex_words' not in text_analysis:\n",
    "        text_analysis['Percentage_of_Complex_words'] = []\n",
    "    text_analysis['Percentage_of_Complex_words'].append(Percentage_of_Complex_words)\n",
    "    if 'Fog_Index' not in text_analysis:\n",
    "        text_analysis['Fog_Index'] = []\n",
    "    text_analysis['Fog_Index'].append(Fog_Index)\n",
    "\n",
    "    file = open(f\"{file_name}.txt\", \"w\")\n",
    "    file.write(cleaned_text)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b5514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for count,url in enumerate(df['URL']):\n",
    "    print(count)\n",
    "    data,text = get_text(url , stopwords)\n",
    "    if text == \"page not found.\":\n",
    "        continue\n",
    "    \n",
    "\n",
    "    save(data,text,count)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a06f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db1b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62c5cd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "for i in text_analysis.keys():\n",
    "    print(len(text_analysis[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "533e7265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['URL_ID', 'Positive_Score', 'Negative_Score', 'POLARITY_SCORE', 'Subjectivity_Score', 'Average_Sentence_Length', 'Percentage_of_Complex_words', 'Fog_Index'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_analysis.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07b21f22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8692/2610031899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_analysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(text_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c80099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
